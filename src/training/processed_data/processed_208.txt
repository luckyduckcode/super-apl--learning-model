<system>Protocol: Active. Attitude: Sassy.</system>
⍝ llama.apl - APL Llama-family transformer implementation
⍝ Uses architecture metadata from generated_manifest.apl

⍝ Load manifest variables (this file is generated by manifest_to_apl.py)
⍝ )LOAD apl/generated_manifest.apl

⍝ ============================================================================
⍝ CONFIGURATION FROM MANIFEST
⍝ These variables are set by generated_manifest.apl:
⍝   MODEL_FAMILY      - 'llama', 'mistral', 'qwen', etc.
⍝   HIDDEN_SIZE       - Model hidden dimension (d_model)
⍝   INTERMEDIATE_SIZE - FFN intermediate dimension  
⍝   NUM_LAYERS        - Number of transformer blocks
⍝   NUM_HEADS         - Number of attention heads
⍝   KV_GROUPS         - KV heads for GQA (= NUM_HEADS for full attention)
⍝   HEAD_DIM          - Per-head dimension
⍝   ATTENTION_VARIANT - 'full', 'gqa', 'mqa', 'sliding-window'
⍝   ACTIVATION        - 'relu', 'swiglu', 'geglu'
⍝   NORM_TYPE         - 'layernorm', 'rmsnorm'
⍝   ROPE_BASE         - RoPE theta base (10000 default)
⍝   ROPE_SCALE        - RoPE scaling factor
⍝ ============================================================================

⍝ --- Utility functions ---

⍝ Softmax along last axis
softmax ← {⍵ ÷ +/⍵ ← *⍵ - ⌈/⍵}

⍝ ReLU activation
relu ← {0 ⌈ ⍵}

⍝ GELU approximation (used in some models)
gelu ← {⍵ × 0.5 × 1 + 2 ○ ⍵ × 0.7978845608 × 1 + 0.044715 × ⍵*2}

⍝ SwiGLU activation (Llama, Mistral, etc.)
⍝ Takes [gate, up] concatenated, returns gate * σ(gate) * up
swiglu ← {
    half ← (⊃⌽⍴⍵) ÷ 2
    gate ← half ↑ ⍵
    up ← half ↓ ⍵
    (gate × ÷ 1 + *-gate) × up   ⍝ SiLU(gate) × up
}

⍝ Select activation by name
get_activation ← {
    ⍵≡'relu': relu
    ⍵≡'swiglu': swiglu
    ⍵≡'geglu': gelu  ⍝ simplified
    relu  ⍝ default
}

⍝ Layer normalization
layernorm ← {gamma omega alpha ← ⍵
    mu ← (+/alpha) ÷ ⍴alpha
    sigma ← ((+/(alpha-mu)*2) ÷ ⍴alpha)*0.5
    gamma × (alpha - mu) ÷ sigma + 1e¯5
}

⍝ RMS normalization (Llama, Mistral)
rmsnorm ← {gamma alpha ← ⍵
    rms ← ((+/alpha*2) ÷ ⍴alpha)*0.5
    gamma × alpha ÷ rms + 1e¯5
}

⍝ Select norm by type
get_norm ← {
    ⍵≡'rmsnorm': rmsnorm
    ⍵≡'layernorm': layernorm
    layernorm  ⍝ default
}

⍝ --- RoPE (Rotary Position Embeddings) ---

⍝ Compute rotation frequencies for position p
rope_freqs ← {p base dim ← ⍵
    half ← dim ÷ 2
    freqs ← base * - (⍳half) ÷ half
    p × freqs
}

⍝ Apply rotation to query/key
apply_rope ← {x pos base ← ⍵
    dim ← ⊃⌽⍴x
    freqs ← rope_freqs pos base dim
    cos_f ← 2○freqs
    sin_f ← 1○freqs
    half ← dim ÷ 2
    x1 ← half ↑ x
    x2 ← half ↓ x
    (x1 × cos_f) - (x2 × sin_f), (x1 × sin_f) + (x2 × cos_f)
}

⍝ --- Attention ---

⍝ Scaled dot-product attention
⍝ Q, K, V are [seq_len, head_dim]
sdp_attention ← {Q K V ← ⍵
    d_k ← ⊃⌽⍴K
    scores ← (Q +.× ⍉K) ÷ d_k * 0.5
    weights ← softmax scores
    weights +.× V
}

⍝ Multi-head attention with optional GQA
⍝ Uses global NUM_HEADS, KV_GROUPS, HEAD_DIM
multihead_attention ← {x Wq Wk Wv Wo pos ← ⍵
    ⍝ Project queries, keys, values
    Q ← x +.× ⍉Wq
    K ← x +.× ⍉Wk  
    V ← x +.× ⍉Wv
    
    ⍝ Reshape to heads: [seq, heads, head_dim]
    seq_len ← ⊃⍴x
    Q ← (seq_len, NUM_HEADS, HEAD_DIM) ⍴ Q
    
    ⍝ For GQA: KV have fewer heads, need to repeat
    kv_heads ← KV_GROUPS
    K ← (seq_len, kv_heads, HEAD_DIM) ⍴ K
    V ← (seq_len, kv_heads, HEAD_DIM) ⍴ V
    
    ⍝ Apply RoPE to Q and K
    Q ← apply_rope Q pos ROPE_BASE
    K ← apply_rope K pos ROPE_BASE
    
    ⍝ Expand KV heads for GQA (repeat each KV head to match Q heads)
    repeat_factor ← NUM_HEADS ÷ kv_heads
    K ← K[;(kv_heads ⍴ ⍳kv_heads) ∘.+ repeat_factor × ⍳1;]
    V ← V[;(kv_heads ⍴ ⍳kv_heads) ∘.+ repeat_factor × ⍳1;]
    
    ⍝ Attention per head
    out ← ⍬
    :For h :In ⍳NUM_HEADS
        head_out ← sdp_attention Q[;h;] K[;h;] V[;h;]
        out ← out, head_out
    :EndFor
    
    ⍝ Concat and project
    out ← (seq_len, NUM_HEADS × HEAD_DIM) ⍴ out
    out +.× ⍉Wo
}

⍝ --- Feedforward Network ---

⍝ FFN with configurable activation
ffn ← {x W1 W2 act_fn ← ⍵
    hidden ← act_fn x +.× ⍉W1
    hidden +.× ⍉W2
}

⍝ SwiGLU FFN (gate and up projection)
swiglu_ffn ← {x Wgate Wup Wdown ← ⍵
    gate ← x +.× ⍉Wgate
    up ← x +.× ⍉Wup
    ⍝ SiLU activation on gate
    silu_gate ← gate × ÷ 1 + *-gate
    hidden ← silu_gate × up
    hidden +.× ⍉Wdown
}

⍝ --- Transformer Layer ---

⍝ Single transformer block (pre-norm architecture)
transformer_layer ← {x layer_weights pos norm_fn act_fn ← ⍵
    ⍝ Unpack layer weights
    Wq Wk Wv Wo W1 W2 norm1_w norm2_w ← layer_weights
    
    ⍝ Pre-norm attention
    normed ← norm_fn norm1_w x
    attn_out ← multihead_attention normed Wq Wk Wv Wo pos
    x ← x + attn_out  ⍝ residual
    
    ⍝ Pre-norm FFN
    normed ← norm_fn norm2_w x
    ffn_out ← ffn normed W1 W2 act_fn
    x + ffn_out  ⍝ residual
}

⍝ --- Full Model ---

⍝ Run transformer forward pass
transformer_forward ← {input_ids weights ← ⍵
    ⍝ Unpack weights
    emb_weight layer_weights output_norm output_proj ← weights
    
    ⍝ Get config functions
    norm_fn ← get_norm NORM_TYPE
    act_fn ← get_activation ACTIVATION
    
    ⍝ Embed tokens
    x ← emb_weight[input_ids;]
    seq_len ← ⊃⍴x
    
    ⍝ Process through layers
    :For i :In ⍳NUM_LAYERS
        pos ← ⍳seq_len  ⍝ position indices
        x ← transformer_layer x (layer_weights[i]) pos norm_fn act_fn
    :EndFor
    
    ⍝ Final norm and projection
    x ← norm_fn output_norm x
    x +.× ⍉output_proj
}

⍝ --- Entry Point ---

⍝ Example usage (requires loaded weights):
⍝   tokens ← 1 2 3 4 5  ⍝ input token IDs
⍝   logits ← transformer_forward tokens MODEL_WEIGHTS

⍝ For binary 1-bit weights, the matmul operations (+.×) would be replaced
⍝ with calls to the backend_1bit shared library via the Python FFI wrapper.

⍝ Print model configuration on load
⎕←'=== Llama APL Model ==='
⎕←'Model Family: ',MODEL_FAMILY
⎕←'Hidden Size: ',⍕HIDDEN_SIZE
⎕←'Layers: ',⍕NUM_LAYERS  
⎕←'Heads: ',⍕NUM_HEADS
⎕←'KV Groups: ',⍕KV_GROUPS
⎕←'Attention: ',ATTENTION_VARIANT
⎕←'Activation: ',ACTIVATION
⎕←'Normalization: ',NORM_TYPE
⎕←'RoPE Base: ',⍕ROPE_BASE
